{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inp_ch, out_ch, kernel_size, padding, stride, activation = 'LeakyReLu') -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(inp_ch,out_ch, kernel_size, padding=2,stride=2)\n",
    "        self.bn = nn.BatchNorm2d(out_ch,momentum=0.9)\n",
    "        if activation == 'LeakyReLu':\n",
    "           self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'Tanh':\n",
    "           self.activation = nn.Tanh()\n",
    "        else:\n",
    "           raise(ValueError, 'Unknown error func')\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "           self.conv,\n",
    "           self.bn,\n",
    "           self.activation\n",
    "          \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class BasicDeconv(nn.Module):\n",
    "    def __init__(self, inp_ch, out_ch, kernel_size, padding, stride, activation = 'LeakyReLu') -> None:\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(inp_ch,out_ch, kernel_size, padding=2,stride=2)\n",
    "        self.bn = nn.BatchNorm2d(out_ch,momentum=0.9)\n",
    "        if activation == 'LeakyReLu':\n",
    "           self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'Tanh':\n",
    "           self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "          self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "           raise(ValueError, 'Unknown error func')\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "           self.deconv,\n",
    "           self.bn,\n",
    "           self.activation\n",
    "          \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.layer1 = BasicBlock(1, 64, 5, padding=2, stride=2)\n",
    "    self.layer2 = BasicBlock(64, 128, 5, padding=2, stride=2)\n",
    "    self.layer3 = BasicBlock(128, 256, 5, padding=2, stride=2)\n",
    "\n",
    "    self.fc1=nn.Linear(256*8*8,2048)\n",
    "    self.bn4=nn.BatchNorm1d(2048,momentum=0.9)\n",
    "    self.fc_mean=nn.Linear(2048,128)\n",
    "    self.fc_logvar=nn.Linear(2048,128)    \n",
    "  \n",
    "  def forward(self,x):\n",
    "    batch_size=x.size()[0]\n",
    "    out = self.layer1(x)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = out.view(batch_size, -1)\n",
    "    out=torch.relu(self.bn4(self.fc1(out)))\n",
    "    mean=self.fc_mean(out)\n",
    "    logvar=self.fc_logvar(out)\n",
    "    return mean,logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Decoder,self).__init__()\n",
    "    self.fc1=nn.Linear(128,8*8*256)\n",
    "    self.bn1=nn.BatchNorm1d(8*8*256,momentum=0.9)\n",
    "    self.relu=nn.LeakyReLU(0.2)\n",
    "\n",
    "    self.deconv1 = BasicDeconv(256,256,6, stride=2, padding=2)\n",
    "    self.deconv2 = BasicDeconv(256,128,6, stride=2, padding=2)\n",
    "    self.deconv3 = BasicDeconv(128,32,6, stride=2, padding=2)\n",
    "    self.deconv4 = BasicDeconv(32,1,5, stride=1, padding=2, activation='Tanh')\n",
    "\n",
    "  def forward(self,x):\n",
    "    batch_size=x.size()[0]\n",
    "    x=self.relu(self.bn1(self.fc1(x)))\n",
    "    out=x.view(-1,256,8,8)\n",
    "    out = self.deconv1(out)\n",
    "    out = self.deconv2(out)\n",
    "    out = self.deconv3(out)\n",
    "    out = self.deconv4(out)\n",
    "    return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator,self).__init__()\n",
    "    self.layer1 = BasicBlock(1, 32, 5, padding=2, stride=1)\n",
    "    self.layer2 = BasicBlock(32, 128, 5, padding=2, stride=2)\n",
    "    self.layer3 = BasicBlock(128, 256, 5, padding=2, stride=2)\n",
    "    self.layer4 = BasicBlock(256, 256, 5, padding=2, stride=2)\n",
    "    self.fc1=nn.Linear(8*8*256,512)\n",
    "    self.bn=nn.BatchNorm1d(512,momentum=0.9)\n",
    "    self.fc2=nn.Linear(512,1)\n",
    "    self.sigmoid=nn.Sigmoid()\n",
    "    self.relu = nn.LeakyReLU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "    x=x.view(-1,256*8*8)\n",
    "    x1=x\n",
    "    x=self.relu(self.bn(self.fc1(x)))\n",
    "    x=self.sigmoid(self.fc2(x))\n",
    "    return x,x1\n",
    "\n",
    "class VAE_GAN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VAE_GAN,self).__init__()\n",
    "    self.encoder = Encoder()\n",
    "    self.decoder = Decoder()\n",
    "    self.encoder.apply(weights_init)\n",
    "    self.decoder.apply(weights_init)\n",
    "\n",
    "  def forward(self,x):\n",
    "    bs = x.size()[0]\n",
    "    z_mean, z_logvar = self.encoder(x)\n",
    "    std = z_logvar.mul(0.5).exp_()\n",
    "    # sampling \n",
    "    epsilon = torch.randn(bs,128).to(device)\n",
    "    z = z_mean + std * epsilon\n",
    "    x_tilda = self.decoder(z)\n",
    "    return z_mean, z_logvar, x_tilda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "class MNIST_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, x):\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = x\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns the tuple (img, None) with the given index.\"\"\"\n",
    "        img = self.data[index]\n",
    "        img = torch.Tensor(np.expand_dims(img, axis=0))/255.\n",
    "        t = transforms.Resize((64, 64))\n",
    "        img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_example(file_name,img):\n",
    "    npimg = np.transpose(img.numpy(),(1,2,0))\n",
    "    f = \"./%s.png\" % file_name\n",
    "    fig = plt.figure(dpi=200)\n",
    "    fig.suptitle(file_name, fontsize=14, fontweight='bold')\n",
    "    plt.imsave(f,npimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_data(path):\n",
    "    with np.load(path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data('3. mnist.npz')\n",
    "train_dataset = MNIST_dataset(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_dataset, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "save_checkpoint = 'checkpoints'\n",
    "\n",
    "# models\n",
    "generator=VAE_GAN().to(device)\n",
    "discrim=Discriminator().to(device)\n",
    "\n",
    "# Параметры обучения\n",
    "epochs=5\n",
    "lr=3e-4\n",
    "alpha=0.1\n",
    "gamma=15\n",
    "\n",
    "# Функции ошибки\n",
    "criterion=nn.BCELoss().to(device)\n",
    "optim_E=torch.optim.Adam(generator.encoder.parameters(), lr=lr)\n",
    "optim_D=torch.optim.Adam(generator.decoder.parameters(), lr=lr)\n",
    "optim_Dis=torch.optim.Adam(discrim.parameters(), lr=lr*alpha)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  encoder_loss_list, gan_loss_list, decoder_loss_list=[],[],[]\n",
    "  dis_real_list,dis_fake_list,dis_noise_list=[],[],[]\n",
    "  i = 0\n",
    "  for i, data in enumerate(tqdm(data_loader), 0):\n",
    "\n",
    "    bs=data.size()[0]\n",
    "    \n",
    "    ones_label=Variable(torch.ones(bs,1)).to(device)\n",
    "    zeros_label=Variable(torch.zeros(bs,1)).to(device)\n",
    "    zeros_label1=Variable(torch.zeros(64,1)).to(device)\n",
    "\n",
    "    x_r = Variable(data).to(device)\n",
    "\n",
    "    # Получаем изображения используя GT\n",
    "    mean, logvar, x_f = generator(x_r)\n",
    "    \n",
    "    # Получаем изображения из Шума\n",
    "    z_p = Variable(torch.randn(64,128)).to(device)\n",
    "    x_p = generator.decoder(z_p)\n",
    "\n",
    "    # Считаем ответы Дискриминатора\n",
    "    output, x_l = discrim(x_r)\n",
    "    errD_real = criterion(output, ones_label)\n",
    "    dis_real_list.append(errD_real.item())\n",
    "\n",
    "    output, x_l_tilda = discrim(x_f)\n",
    "    errD_fake = criterion(output, zeros_label)\n",
    "    dis_fake_list.append(errD_fake.item())\n",
    "\n",
    "    output = discrim(x_p)[0]\n",
    "    errD_noise = criterion(output, zeros_label1)\n",
    "    dis_noise_list.append(errD_noise.item())\n",
    "\n",
    "\n",
    "    # Обучаем дискриминатор\n",
    "    gan_loss = errD_real + errD_fake + errD_noise\n",
    "    gan_loss_list.append(gan_loss.item())\n",
    "    optim_Dis.zero_grad()\n",
    "    gan_loss.backward(retain_graph=True)\n",
    "    optim_Dis.step()\n",
    "    ##############################\n",
    "    \n",
    "    # Ответы нового дискриминатора\n",
    "    output, x_l = discrim(x_r)\n",
    "    errD_real = criterion(output, ones_label)\n",
    "\n",
    "    output, x_l_tilda = discrim(x_f)\n",
    "    errD_rec_enc = criterion(output, zeros_label)\n",
    "    output = discrim(x_p)[0]\n",
    "    errD_rec_noise = criterion(output, zeros_label1)\n",
    "    gan_loss = errD_real + errD_rec_enc + errD_rec_noise\n",
    "    \n",
    "    # Обучаем Декодер\n",
    "    rec_loss = ((x_l_tilda - x_l) ** 2).mean()\n",
    "    err_dec = gamma * rec_loss - gan_loss \n",
    "    encoder_loss_list.append(rec_loss.item())\n",
    "    optim_D.zero_grad()\n",
    "    err_dec.backward(retain_graph=True)\n",
    "    optim_D.step()\n",
    "    \n",
    "    # Обучаем Энкодер\n",
    "    mean, logvar, x_f = generator(x_r)\n",
    "    x_l_tilda = discrim(x_f)[1]\n",
    "    x_l = discrim(x_r)[1]\n",
    "    rec_loss = ((x_l_tilda - x_l) ** 2).mean()\n",
    "    prior_loss = 1 + logvar - mean.pow(2) - logvar.exp()\n",
    "    prior_loss = (-0.5 * torch.sum(prior_loss))/torch.numel(mean.data)\n",
    "    encoder_loss_list.append(prior_loss.item())\n",
    "    err_enc = prior_loss + 5*rec_loss\n",
    "\n",
    "    optim_E.zero_grad()\n",
    "    err_enc.backward(retain_graph=True)\n",
    "    optim_E.step()\n",
    "        \n",
    "  print('Loss_gan: %.4f\\tEncoder_loss: %.4f\\tDecoder_loss: %.4f\\tdis_real_loss: %0.4f\\tdis_fake_loss: %.4f\\tdis_prior_loss: %.4f'\n",
    "                  % (np.mean(gan_loss_list), np.mean(encoder_loss_list),np.mean(encoder_loss_list), np.mean(dis_real_list), np.mean(dis_fake_list), np.mean(dis_noise_list)))\n",
    "  \n",
    "  # Генерация примеров\n",
    "  real_batch = next(iter(data_loader))\n",
    "  z_fixed=Variable(torch.randn((64,128))).to(device)\n",
    "  x_fixed=Variable(real_batch[0]).to(device)\n",
    "  b = generator(x_fixed)[2]\n",
    "  b = b.detach()\n",
    "  c = generator.decoder(z_fixed)\n",
    "  c = c.detach()\n",
    "  save_example('MNISTrec_noise_epoch_%d.png' % epoch ,make_grid((c*0.5+0.5).cpu(),8))\n",
    "  save_example('MNISTrec_epoch_%d.png' % epoch,make_grid((b*0.5+0.5).cpu(),8))\n",
    "  torch.save(generator.state_dict(), os.path.join(save_checkpoint, 'gen_'+str(epoch)+'.pt'))\n",
    "  torch.save(discrim.state_dict(), os.path.join(save_checkpoint, 'disc_'+str(epoch)+'.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_virt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
